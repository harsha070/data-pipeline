# define properties for sa_events file
sa_events:
  # path for sa_events file (supports url or file pointing to a csv, tsv, xlsx)
  source: 'https://raw.githubusercontent.com/harsha070/data-pipeline/048d989b592c374d4b88bb878fac3c08d3b73c64/data/sa_events.xlsx'


# define properties for bmt events file
bmt_events:
  # path for bmt_events file (supports url or file pointing to a csv, tsv, xlsx)
  source: 'https://raw.githubusercontent.com/harsha070/data-pipeline/048d989b592c374d4b88bb878fac3c08d3b73c64/data/bmt_events.tsv'


# path for fsym_to_ticker mapping file (supports url or file pointing to a csv, tsv, xlsx)
fsym: 'https://raw.githubusercontent.com/harsha070/data-pipeline/048d989b592c374d4b88bb878fac3c08d3b73c64/data/fsym_to_ticker.tsv'


# flag to upload output events to s3. if set to True, pass aws config (s3_access_key and s3_secret_key) below
upload_to_s3: False


# define your AWS credentials
aws:
  s3_access_key: admin
  s3_secret_key: password


# cron job frequency (in minutes). Set to run the data pipeline runs every 1 minute.
job_interval: 1 # minutes

